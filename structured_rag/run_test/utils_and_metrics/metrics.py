import json
from typing import Any, Tuple

# This needs a refactor to validate based on the models
def is_valid_json_output(output: Any, test_type: str) -> bool:
    try:
        parsed = json.loads(output)
        if test_type == "GenerateAnswer":
            return isinstance(parsed.get("answer"), str)
        elif test_type == "RateContext":
            score = parsed.get("context_score")
            return isinstance(score, int) and 0 <= score <= 5
        elif test_type == "AssessAnswerability":
            return isinstance(parsed.get("answerable_question"), bool)
        elif test_type == "ParaphraseQuestions":
            questions = parsed.get("paraphrased_questions")
            return isinstance(questions, list) and all(isinstance(q, str) for q in questions)
        elif test_type == "RAGAS":
            faithfulness_score = parsed.get("faithfulness_score")
            answer_relevance_score = parsed.get("answer_relevance_score")
            context_relevance_score = parsed.get("context_relevance_score")
            return isinstance(faithfulness_score, float) and isinstance(answer_relevance_score, float) and isinstance(context_relevance_score, float) and 0 <= faithfulness_score <= 5 and 0 <= answer_relevance_score <= 5 and 0 <= context_relevance_score <= 5
        elif test_type == "GenerateAnswerWithConfidence":
            return isinstance(parsed.get("Answer"), str) and isinstance(parsed.get("Confidence"), int) and 0 <= parsed["Confidence"] <= 5
        elif test_type == "GenerateAnswersWithConfidence":
            answers = parsed
            return isinstance(answers, list) and all(isinstance(a.get("Answer"), str) and isinstance(a.get("Confidence"), int) and 0 <= a["Confidence"] <= 5 for a in answers)
        # ToDo, fix these to also test the Enum value
        elif test_type == "ClassifyDocument":
            return isinstance(parsed.get('category'), str)
        elif test_type == "ClassifyDocumentWithRationale":
            return isinstance(parsed.get('category'), str) and isinstance(parsed.get('rationale'), str)

        else:
            return False
    except json.JSONDecodeError:
        return False

# Although assess_answerability_metric and classification_metric currently do the same thing,
# ==> we want to extend classification_metric in the future to put probabilties on more than one class.
# ==> and thus we will extend this later on as described.

def assess_answerability_metric(answer: bool, ground_truth: bool) -> int:
    if answer == ground_truth:
        return 1
    else:
        return 0

def classification_metric(predicted_class: str, ground_truth: str) -> int:
    if predicted_class == ground_truth:
        return 1
    else:
        return 0

import dspy

class AssessAnswerAlignment(dspy.Signature):
    """Assess the alignment between the system answer and the ground truth answer on a scale of 0 to 5."""

    context: str = dspy.InputField(description="The context to use for answering the question.")
    question: str = dspy.InputField(description="The question to answer.")
    system_answer: str = dspy.InputField(description="The answer generated by the system.")
    ground_truth: str = dspy.InputField(description="The ground truth answer.")
    score_rationale: str = dspy.OutputField(description="The rationale for the alignment score. Please make it very clear why you chose this particular score and not the others.")
    alignment_score: int = dspy.OutputField(description="The alignment score on an integer scale of 0 to 5 between the system answer and the ground truth answer. 0 meaning the system answer is not aligned with the ground truth answer, 5 meaning the system answer is fully aligned with the ground truth answer.")

assess_answer_alignment = dspy.TypedPredictor(AssessAnswerAlignment)

class GenerateAnswerTaskMetric:
    def __init__(self, api_key: str):
        self.gpt4 = dspy.OpenAI(model="gpt-4o", api_key=api_key)
        self.assess_answer_alignment = assess_answer_alignment

    def assess_answer_metric(self, context: str, question: str, system_answer: str, ground_truth: str) -> Tuple[int, str]:
        with dspy.context(lm=self.gpt4):
            metric_output = self.assess_answer_alignment(context=context, question=question, system_answer=system_answer, ground_truth=ground_truth)
        return metric_output.alignment_score, metric_output.score_rationale