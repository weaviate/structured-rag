import json
from typing import Any, Tuple

# This needs a refactor to validate based on the models

def _validate_int_score(score, min_val=0, max_val=5):
    if isinstance(score, str):
        try:
            score = int(score)
        except ValueError:
            return None, False
    return score, isinstance(score, int) and min_val <= score <= max_val

def _validate_float_score(score, min_val=0, max_val=5):
    if isinstance(score, str):
        try:
            score = float(score)
        except ValueError:
            return None, False
    return score, isinstance(score, float) and min_val <= score <= max_val

def _validate_boolean(value):
    if isinstance(value, bool):
        return value, True
    if isinstance(value, str):
        lower_value = value.lower()
        if lower_value in ['true', 'false']:
            return lower_value == 'true', True
    return None, False

def is_valid_json_output(output: Any, test_type: str) -> Tuple[Any, bool]:
    try:
        parsed = json.loads(output)
        if test_type == "GenerateAnswer":
            answer = parsed.get("answer")
            return parsed, isinstance(answer, str)
        elif test_type == "RateContext":
            score, is_valid = _validate_int_score(parsed.get("context_score"))
            return parsed if is_valid else None, is_valid
        elif test_type == "AssessAnswerability":
            answerable, is_valid = _validate_boolean(parsed.get("answerable_question"))
            return answerable if is_valid else None, is_valid
        elif test_type == "ParaphraseQuestions":
            questions = parsed.get("paraphrased_questions")
            is_valid = isinstance(questions, list) and all(isinstance(q, str) for q in questions)
            return parsed if is_valid else None, is_valid
        elif test_type == "RAGAS":
            scores = ["faithfulness_score", "answer_relevance_score", "context_relevance_score"]
            is_valid = all(_validate_float_score(parsed.get(score))[1] for score in scores)
            return parsed if is_valid else None, is_valid
        elif test_type == "GenerateAnswerWithConfidence":
            answer_valid = isinstance(parsed.get("Answer"), str)
            confidence, confidence_valid = _validate_int_score(parsed.get("Confidence"))
            return parsed if answer_valid and confidence_valid else None, answer_valid and confidence_valid
        elif test_type == "GenerateAnswersWithConfidence":
            answers = parsed
            is_valid = isinstance(answers, list) and all(
                isinstance(a.get("Answer"), str) and _validate_int_score(a.get("Confidence"))[1]
                for a in answers
            )
            return parsed if is_valid else None, is_valid
        elif test_type == "ClassifyDocument":
            is_valid = isinstance(parsed.get('category'), str)
            return parsed if is_valid else None, is_valid
        elif test_type == "ClassifyDocumentWithRationale":
            is_valid = isinstance(parsed.get('category'), str) and isinstance(parsed.get('rationale'), str)
            return parsed if is_valid else None, is_valid
        else:
            return None, False
    except json.JSONDecodeError:
        return None, False

# Although assess_answerability_metric and classification_metric currently do the same thing,
# ==> we want to extend classification_metric in the future to put probabilties on more than one class.
# ==> and thus we will extend this later on as described.

def assess_answerability_metric(answer: bool, ground_truth: bool) -> int:
    if answer == ground_truth:
        return 1
    else:
        return 0

def classification_metric(predicted_class: str, ground_truth: str) -> int:
    if predicted_class == ground_truth:
        return 1
    else:
        return 0

import dspy

class AssessAnswerAlignment(dspy.Signature):
    """Assess the alignment between the system answer and the ground truth answer on a scale of 0 to 5."""

    context: str = dspy.InputField(description="The context to use for answering the question.")
    question: str = dspy.InputField(description="The question to answer.")
    system_answer: str = dspy.InputField(description="The answer generated by the system.")
    ground_truth: str = dspy.InputField(description="The ground truth answer.")
    score_rationale: str = dspy.OutputField(description="The rationale for the alignment score. Please make it very clear why you chose this particular score and not the others.")
    alignment_score: int = dspy.OutputField(description="The alignment score on an integer scale of 0 to 5 between the system answer and the ground truth answer. 0 meaning the system answer is not aligned with the ground truth answer, 5 meaning the system answer is fully aligned with the ground truth answer.")

assess_answer_alignment = dspy.TypedPredictor(AssessAnswerAlignment)

class GenerateAnswerTaskMetric:
    def __init__(self, api_key: str):
        self.gpt4 = dspy.OpenAI(model="gpt-4o", api_key=api_key)
        self.assess_answer_alignment = assess_answer_alignment

    def assess_answer_metric(self, context: str, question: str, system_answer: str, ground_truth: str) -> Tuple[int, str]:
        with dspy.context(lm=self.gpt4):
            metric_output = self.assess_answer_alignment(context=context, question=question, system_answer=system_answer, ground_truth=ground_truth)
        return metric_output.alignment_score, metric_output.score_rationale